# Word Embedding
Word Embedding is the process of converting words or text or tokens into dense vector representation.

The idea is to represent each toke in a high-dimensional space where similar words are closer together.

## Steps to convert TOKENS into VECTOR representation
- Tokenization
- Vocabulary creation
- Embeddding Matrix
- Lookup Operation
- Embedding Output

## Algorithms discussed
There are many algorithms to convert tokens into vectors. We will look into the following:
- One-Hot Encoding
- Word2vec
- Glove
- FastText
- ELMo
- BERT
- GPT embeddings
